웹 크롤러 설계
=

웹 크롤러는 검색엔진의 인덱싱, 웹 아카이빙, 웹 마이닝, 웹 모니터링 등의 목적으로 사용된다.

1. URL 집합이 입력으로 주어지면 해당 URL이 가리키는 모든 웹 페이지를 다운로드한다.
2. 다운받은 웹 페이지에서 URL들을 추출한다.
3. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 반복한다.

병렬성을 사용한 규모 확장성, 비정상적인 입력에 대한 안정성, 요청 간격, 컨텐츠 양식의 확장성 등을 고려해보아야 한다.

웹 크롤러의 기본 알고리즘은 간단하다.

페이지는 노드이고 하이퍼링크는 엣지이다. 크롤링은 이 유향 그래프를 에지에 따라 탐색하는 과정이다.
웹 크롤러는 보통 BFS를 사용한다.
FIFO 큐에 한 쪽으로는 URL을 집어넣고, 한 쪽으로는 꺼내기만 한다.
하지만 한 페이지에서 나오는 링크의 상당수는 같은 서버로 돌아가므로, 자칫 같은 페이지에 너무 많은 요청을 몰아 할 여지가 있다.
또 BFS는 URL 사이의 우선순위를 두지 않는다는 점도 고려해야 한다.

미수집 저장소를 잘 구현하면 우선순위와 신선도, 그리고 요청 간격을 고려한 크롤러를 만들 수 있다.

성능 최적화
-

- 분산 크롤링
- 도메인 이름 변환 결과 캐시
- 지역성 활용
- 짧은 타임아웃

안정성
-

- 안정해시
- 크롤링 상태 및 수집 데이터 저장
- 예외처리
- 데이터 검증
