웹 크롤러 설계
=

웹 크롤러는 [검색엔진의 인덱싱](https://julie-tech.tistory.com/103), 웹 아카이빙, 웹 마이닝, 웹 모니터링 등의 목적으로 사용된다.

1. 병렬성을 사용한 규모 확장성
2. 비정상적인 입력에 대한 안정성
3. 요청 간격
4. 컨텐츠 양식의 확장성

등을 고려해보아야 한다.

웹 크롤러의 기본 알고리즘
-

웹 크롤러의 **기본 알고리즘**은 간단하다.

1. URL 집합이 입력으로 주어지면 **해당 URL이 가리키는 모든 웹 페이지를 다운로드**한다. (동적 URL에 대해서는 SSR도 고려)
2. 다운받은 웹 페이지에서 **URL들을 추출**한다.
3. **추출된 URL들을 다운로드할 URL 목록에 추가**하고 위의 과정을 **반복**한다.

페이지는 노드이고 하이퍼링크는 엣지이다. 크롤링은 이 유향 그래프를 에지에 따라 탐색하는 과정이다.
웹 크롤러는 보통 **BFS**를 사용한다.
FIFO 큐에 한 쪽으로는 URL을 집어넣고, 한 쪽으로는 꺼내기만 한다.

**하지만** 한 페이지에서 나오는 링크의 상당수는 같은 서버로 돌아가므로, 자칫 같은 페이지에 **너무 많은 요청**을 몰아 할 여지가 있다.
또 **BFS는 URL 사이의 우선순위를 두지 않는다**는 점도 고려해야 한다.

미수집 저장소를 잘 구현하면 **우선순위**와 **신선도**, 그리고 **요청 간격**을 고려한 크롤러를 만들 수 있다. 

- 우선순위 결정장치는 우선순위 큐를 두어 사용할 수 있다.
- 웹 사이트의 호스트와 다운로드를 수행할 작업 스레드 사이에 별도의 큐를 두고 일정한 지연을 두는 방법을 고려할 수 있다.
- 신선도는 웹 페이지의 변경이력, 우선 순위 등을 고려하여 재크롤링을 할 수 있다.


개략적인 구조
-

<img width="775" alt="스크린샷 2024-04-13 오후 1 52 50" src="https://github.com/bosungpark/systemdesign-markdown-document/assets/81157873/36abdfde-e39a-4feb-a3ef-d96afadb6788">

- **시작 URL 집합**: 정답은 없다. 도메인의 이름이 붙은 URL로 쓸 수도, 작은 부분집합으로 주제를 나누어 탐색하는 것도 자유롭게 고려하면 된다.
- **미수집 URL 저장소**: 다운로드할 URL을 저장하는 저장소이다. FIFO 큐이다.
- **콘텐츠 파서**: 다운로드한 웹 페이지는 파싱과 검증의 절차를 거쳐야 하는데, 자칫 이상한 페이지가 문제를 일으킰수도 있고, 저장공간만 잡아 먹을 수 있기 때문. 크롤링의 속도를 고려해 독립된 컴포넌트로 분리
- **중복 컨텐츠 검증**: 해시값 혹은 체크섬을 비교하는 것이 효율적인 방식
- **URL 추출기**: HTML 페이지를 파싱하여 링크를 골라내는 역할. 상대경로는 절대경로로 추출
- **URL 필터**: 문제가 될 여지가 있는 URL을 필터링(예: 파일 확장자, 접근 제외 목록, 거미덫 등)
-  **이미 방문한 URL**: 블룸 필터 혹은 해시 테이블 사용



성능 최적화
-

- 분산 크롤링
- 도메인 이름 변환 결과 캐시
- 지역성 활용
- 짧은 타임아웃

안정성
-

- 안정해시
- 크롤링 상태 및 수집 데이터 저장
- 예외처리
- 데이터 검증

확장성
-

- 특별한 건 없고 확장 모듈을 고려하는 방법